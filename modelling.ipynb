{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9484a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1546 > 512). Running this sequence through the model will result in indexing errors\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\xianl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\xianl\\AppData\\Local\\Temp\\ipykernel_16420\\2001217518.py:326: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/94 [00:00<?, ?it/s]C:\\Users\\xianl\\AppData\\Local\\Temp\\ipykernel_16420\\2001217518.py:238: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|██████████| 94/94 [00:21<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5747, Accuracy: 0.7253\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5487, Accuracy: 0.7527\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5258, Accuracy: 0.7520\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4722, Accuracy: 0.7694\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4232, Accuracy: 0.8068\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4035, Accuracy: 0.8242\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4072, Accuracy: 0.8235\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4064, Accuracy: 0.8229\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4057, Accuracy: 0.8249\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4035, Accuracy: 0.8235\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4047, Accuracy: 0.8229\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4048, Accuracy: 0.8209\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4056, Accuracy: 0.8215\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4063, Accuracy: 0.8222\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4058, Accuracy: 0.8249\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4043, Accuracy: 0.8309\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4036, Accuracy: 0.8235\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4048, Accuracy: 0.8222\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4042, Accuracy: 0.8215\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4035, Accuracy: 0.8242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 23/23 [00:04<00:00,  4.93it/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:20<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5676, Accuracy: 0.7317\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:20<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5599, Accuracy: 0.7510\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:20<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5382, Accuracy: 0.7610\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:20<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5339, Accuracy: 0.7669\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:20<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5114, Accuracy: 0.7729\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:20<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5106, Accuracy: 0.7742\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:19<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5113, Accuracy: 0.7749\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:20<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5140, Accuracy: 0.7742\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:19<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5099, Accuracy: 0.7749\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:20<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5075, Accuracy: 0.7736\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:20<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5064, Accuracy: 0.7736\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:20<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5137, Accuracy: 0.7742\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:20<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5055, Accuracy: 0.7742\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:20<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5123, Accuracy: 0.7736\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:20<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5121, Accuracy: 0.7749\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:20<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5134, Accuracy: 0.7749\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:20<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5090, Accuracy: 0.7742\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:20<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5130, Accuracy: 0.7742\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:20<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5056, Accuracy: 0.7742\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:20<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5045, Accuracy: 0.7749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 22/22 [00:04<00:00,  4.98it/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:19<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5672, Accuracy: 0.7424\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:19<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5399, Accuracy: 0.7576\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:19<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5337, Accuracy: 0.7611\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:19<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5205, Accuracy: 0.7611\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:19<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5036, Accuracy: 0.7632\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:19<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4975, Accuracy: 0.7645\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:19<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4952, Accuracy: 0.7652\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:19<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4933, Accuracy: 0.7652\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:19<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4959, Accuracy: 0.7652\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:19<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4987, Accuracy: 0.7645\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:19<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4978, Accuracy: 0.7652\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:19<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4944, Accuracy: 0.7645\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:19<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4957, Accuracy: 0.7632\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:19<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4940, Accuracy: 0.7639\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:19<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4925, Accuracy: 0.7645\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:19<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4971, Accuracy: 0.7625\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:19<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4944, Accuracy: 0.7625\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:19<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4927, Accuracy: 0.7632\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:19<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4986, Accuracy: 0.7652\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:19<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5006, Accuracy: 0.7659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 26/26 [00:05<00:00,  4.99it/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:19<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5878, Accuracy: 0.7285\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:19<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5682, Accuracy: 0.7400\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:19<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5512, Accuracy: 0.7400\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:19<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5276, Accuracy: 0.7387\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:19<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4924, Accuracy: 0.7481\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:19<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4790, Accuracy: 0.7515\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:19<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4818, Accuracy: 0.7508\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:19<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4741, Accuracy: 0.7556\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:19<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4760, Accuracy: 0.7522\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:19<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4762, Accuracy: 0.7542\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:19<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4770, Accuracy: 0.7536\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:19<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4777, Accuracy: 0.7542\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:19<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4754, Accuracy: 0.7542\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:19<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4747, Accuracy: 0.7502\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:19<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4779, Accuracy: 0.7549\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:19<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4749, Accuracy: 0.7549\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:19<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4764, Accuracy: 0.7508\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:19<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4768, Accuracy: 0.7529\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:19<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4755, Accuracy: 0.7549\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:19<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4777, Accuracy: 0.7529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 24/24 [00:04<00:00,  5.02it/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:19<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5798, Accuracy: 0.7288\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5625, Accuracy: 0.7415\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5467, Accuracy: 0.7415\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5284, Accuracy: 0.7375\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4953, Accuracy: 0.7508\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4829, Accuracy: 0.7669\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:19<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4836, Accuracy: 0.7689\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4830, Accuracy: 0.7649\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4834, Accuracy: 0.7642\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4842, Accuracy: 0.7642\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4854, Accuracy: 0.7662\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:19<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4844, Accuracy: 0.7669\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:19<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4841, Accuracy: 0.7682\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4851, Accuracy: 0.7615\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4841, Accuracy: 0.7675\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4839, Accuracy: 0.7655\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4850, Accuracy: 0.7622\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4836, Accuracy: 0.7642\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4828, Accuracy: 0.7615\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:20<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4843, Accuracy: 0.7655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 23/23 [00:04<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== albert-base-chinese on text ===\n",
      "AUC: 0.7138\n",
      "F1: 0.7780\n",
      "Sensitivity: 0.9684\n",
      "Specificity: 0.0917\n",
      "PPV: 0.6502\n",
      "NPV: 0.6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1546 > 512). Running this sequence through the model will result in indexing errors\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indiejoseph/bert-base-cantonese and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\xianl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\xianl\\AppData\\Local\\Temp\\ipykernel_16420\\2001217518.py:326: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: indiejoseph/bert-base-cantonese, Fold: 1, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/94 [00:00<?, ?it/s]C:\\Users\\xianl\\AppData\\Local\\Temp\\ipykernel_16420\\2001217518.py:238: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5691, Accuracy: 0.7373\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 1, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5086, Accuracy: 0.7721\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 1, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4170, Accuracy: 0.8255\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 1, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3036, Accuracy: 0.8803\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 1, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2216, Accuracy: 0.9184\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 1, Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1970, Accuracy: 0.9338\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 1, Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2000, Accuracy: 0.9332\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 1, Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1937, Accuracy: 0.9332\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 1, Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1983, Accuracy: 0.9398\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 1, Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1939, Accuracy: 0.9332\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 1, Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1988, Accuracy: 0.9412\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 1, Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1960, Accuracy: 0.9332\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 1, Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1971, Accuracy: 0.9325\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 1, Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2005, Accuracy: 0.9338\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 1, Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1997, Accuracy: 0.9325\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 1, Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1924, Accuracy: 0.9412\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 1, Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1915, Accuracy: 0.9325\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 1, Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2007, Accuracy: 0.9285\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 1, Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1944, Accuracy: 0.9352\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 1, Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2007, Accuracy: 0.9311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 23/23 [00:04<00:00,  4.98it/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indiejoseph/bert-base-cantonese and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: indiejoseph/bert-base-cantonese, Fold: 2, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5648, Accuracy: 0.7477\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 2, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5346, Accuracy: 0.7722\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 2, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4923, Accuracy: 0.7855\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 2, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4544, Accuracy: 0.8101\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 2, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3964, Accuracy: 0.8300\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 2, Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3713, Accuracy: 0.8506\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 2, Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3878, Accuracy: 0.8473\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 2, Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3722, Accuracy: 0.8519\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 2, Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3813, Accuracy: 0.8433\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 2, Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3793, Accuracy: 0.8426\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 2, Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3740, Accuracy: 0.8440\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 2, Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3783, Accuracy: 0.8453\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 2, Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3776, Accuracy: 0.8486\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 2, Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3734, Accuracy: 0.8473\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 2, Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3784, Accuracy: 0.8473\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 2, Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3739, Accuracy: 0.8440\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 2, Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3760, Accuracy: 0.8426\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 2, Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3738, Accuracy: 0.8446\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 2, Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3731, Accuracy: 0.8426\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 2, Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3749, Accuracy: 0.8433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 22/22 [00:04<00:00,  4.90it/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indiejoseph/bert-base-cantonese and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: indiejoseph/bert-base-cantonese, Fold: 3, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:24<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5636, Accuracy: 0.7493\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 3, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:24<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4604, Accuracy: 0.7922\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 3, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:24<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3753, Accuracy: 0.8317\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 3, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:24<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2634, Accuracy: 0.9003\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 3, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:24<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2015, Accuracy: 0.9217\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 3, Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:24<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1662, Accuracy: 0.9432\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 3, Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:24<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1643, Accuracy: 0.9446\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 3, Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:24<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1699, Accuracy: 0.9453\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 3, Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:24<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1700, Accuracy: 0.9398\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 3, Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:24<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1696, Accuracy: 0.9404\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 3, Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:24<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1733, Accuracy: 0.9370\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 3, Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:24<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1631, Accuracy: 0.9418\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 3, Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:24<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1634, Accuracy: 0.9432\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 3, Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:24<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1651, Accuracy: 0.9432\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 3, Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:24<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1685, Accuracy: 0.9384\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 3, Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:24<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1698, Accuracy: 0.9349\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 3, Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:24<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1741, Accuracy: 0.9363\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 3, Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:24<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1671, Accuracy: 0.9384\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 3, Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:24<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1698, Accuracy: 0.9391\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 3, Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:24<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1816, Accuracy: 0.9404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 26/26 [00:05<00:00,  4.92it/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indiejoseph/bert-base-cantonese and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: indiejoseph/bert-base-cantonese, Fold: 4, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:24<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5666, Accuracy: 0.7339\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 4, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5057, Accuracy: 0.7718\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 4, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4330, Accuracy: 0.8111\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 4, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:24<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3327, Accuracy: 0.8639\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 4, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:24<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2819, Accuracy: 0.8917\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 4, Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:24<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2510, Accuracy: 0.9052\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 4, Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:24<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2582, Accuracy: 0.9045\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 4, Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:24<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2520, Accuracy: 0.9018\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 4, Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:25<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2572, Accuracy: 0.8991\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 4, Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2496, Accuracy: 0.9059\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 4, Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:24<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2597, Accuracy: 0.8978\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 4, Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2484, Accuracy: 0.9066\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 4, Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2503, Accuracy: 0.9012\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 4, Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:24<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2641, Accuracy: 0.8978\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 4, Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:24<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2598, Accuracy: 0.8978\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 4, Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2487, Accuracy: 0.9072\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 4, Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:24<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2550, Accuracy: 0.9039\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 4, Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2516, Accuracy: 0.9086\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 4, Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2456, Accuracy: 0.9093\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 4, Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [00:24<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2552, Accuracy: 0.9012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 24/24 [00:04<00:00,  4.93it/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indiejoseph/bert-base-cantonese and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: indiejoseph/bert-base-cantonese, Fold: 5, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5837, Accuracy: 0.7208\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 5, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5615, Accuracy: 0.7448\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 5, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4973, Accuracy: 0.7695\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 5, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4113, Accuracy: 0.8176\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 5, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3521, Accuracy: 0.8564\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 5, Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3301, Accuracy: 0.8657\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 5, Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3354, Accuracy: 0.8664\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 5, Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3365, Accuracy: 0.8631\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 5, Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3340, Accuracy: 0.8717\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 5, Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3312, Accuracy: 0.8671\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 5, Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3391, Accuracy: 0.8611\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 5, Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3317, Accuracy: 0.8657\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 5, Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3314, Accuracy: 0.8697\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 5, Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3339, Accuracy: 0.8731\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 5, Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3347, Accuracy: 0.8684\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 5, Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3357, Accuracy: 0.8617\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 5, Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3348, Accuracy: 0.8624\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 5, Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3330, Accuracy: 0.8671\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 5, Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3397, Accuracy: 0.8624\n",
      "Model: indiejoseph/bert-base-cantonese, Fold: 5, Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3382, Accuracy: 0.8604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 23/23 [00:04<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== bert-base-cantonese on text ===\n",
      "AUC: 0.8103\n",
      "F1: 0.7927\n",
      "Sensitivity: 0.7947\n",
      "Specificity: 0.6330\n",
      "PPV: 0.7906\n",
      "NPV: 0.6389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zwzzz/Chinese-MentalBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\xianl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\xianl\\AppData\\Local\\Temp\\ipykernel_16420\\2001217518.py:326: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: zwzzz/Chinese-MentalBERT, Fold: 1, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/94 [00:00<?, ?it/s]C:\\Users\\xianl\\AppData\\Local\\Temp\\ipykernel_16420\\2001217518.py:238: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5701, Accuracy: 0.7473\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 1, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5741, Accuracy: 0.7386\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 1, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5366, Accuracy: 0.7433\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 1, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5229, Accuracy: 0.7480\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 1, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5161, Accuracy: 0.7527\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 1, Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5036, Accuracy: 0.7520\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 1, Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5010, Accuracy: 0.7527\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 1, Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5026, Accuracy: 0.7527\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 1, Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5000, Accuracy: 0.7533\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 1, Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4995, Accuracy: 0.7540\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 1, Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5052, Accuracy: 0.7527\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 1, Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5012, Accuracy: 0.7527\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 1, Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5041, Accuracy: 0.7527\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 1, Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5029, Accuracy: 0.7520\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 1, Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5039, Accuracy: 0.7540\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 1, Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5036, Accuracy: 0.7527\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 1, Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5083, Accuracy: 0.7520\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 1, Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5011, Accuracy: 0.7533\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 1, Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4994, Accuracy: 0.7547\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 1, Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:25<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5033, Accuracy: 0.7533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 23/23 [00:04<00:00,  5.02it/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zwzzz/Chinese-MentalBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: zwzzz/Chinese-MentalBERT, Fold: 2, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5747, Accuracy: 0.7384\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 2, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5672, Accuracy: 0.7463\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 2, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5608, Accuracy: 0.7483\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 2, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5394, Accuracy: 0.7430\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 2, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5205, Accuracy: 0.7497\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 2, Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5001, Accuracy: 0.7550\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 2, Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5067, Accuracy: 0.7543\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 2, Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:25<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5002, Accuracy: 0.7610\n",
      "Model: zwzzz/Chinese-MentalBERT, Fold: 2, Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 60/95 [00:16<00:09,  3.64it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 330\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Fold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 330\u001b[0m     train_acc, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    333\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 250\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, data_loader, optimizer, scheduler, device, scaler)\u001b[0m\n\u001b[0;32m    248\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    249\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    252\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\xianl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\amp\\grad_scaler.py:457\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    455\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 457\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    459\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\xianl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\xianl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\amp\\grad_scaler.py:351\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    350\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    352\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    AutoModel, AdamW, get_scheduler, set_seed\n",
    ")\n",
    "from torch.nn import functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy.stats import mannwhitneyu\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Seed setup\n",
    "def seed_everything(seed=6):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(6)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset\n",
    "# class TextDataset(Dataset):\n",
    "#     def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "#         self.texts = texts\n",
    "#         self.labels = labels\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_len = max_len\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         encoding = self.tokenizer.encode_plus(\n",
    "#             self.texts[idx],\n",
    "#             add_special_tokens=True,\n",
    "#             max_length=self.max_len,\n",
    "#             padding=\"max_length\",\n",
    "#             truncation=True,\n",
    "#             return_attention_mask=True,\n",
    "#             return_tensors=\"pt\",\n",
    "#         )\n",
    "#         return {\n",
    "#             \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "#             \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "#             \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "#         }\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512, overlap=128, cases=None):\n",
    "        self.samples = []\n",
    "\n",
    "        for idx, (text, label) in enumerate(zip(texts, labels)):\n",
    "            case_id = cases[idx] if cases is not None else idx\n",
    "            encoding = tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                return_attention_mask=False,\n",
    "                return_tensors=None,\n",
    "            )\n",
    "            input_ids = encoding[\"input_ids\"]\n",
    "            \n",
    "            # Chunking\n",
    "            start = 0\n",
    "            while start < len(input_ids):\n",
    "                end = start + max_len\n",
    "                chunk = input_ids[start:end]\n",
    "                \n",
    "                if len(chunk) < max_len:\n",
    "                    chunk += [tokenizer.pad_token_id] * (max_len - len(chunk))\n",
    "                \n",
    "                self.samples.append({\n",
    "                    \"input_ids\": chunk,\n",
    "                    \"label\": label,\n",
    "                    \"case_id\": case_id,\n",
    "                    \"chunk_id\": start // (max_len - overlap)  # optional: which chunk in that case\n",
    "                })\n",
    "\n",
    "                if end >= len(input_ids):\n",
    "                    break\n",
    "                start += max_len - overlap\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        input_ids = torch.tensor(sample[\"input_ids\"], dtype=torch.long)\n",
    "        attention_mask = (input_ids != 0).long()  # assuming pad_token_id = 0\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"label\": torch.tensor(sample[\"label\"], dtype=torch.long),\n",
    "            \"case_id\": sample[\"case_id\"],\n",
    "            \"chunk_id\": sample[\"chunk_id\"],  # 可用于后续组合分析\n",
    "        }\n",
    "\n",
    "\n",
    "# Fallback for models without classification head\n",
    "class CustomClassifier(torch.nn.Module):\n",
    "    def __init__(self, base_model, hidden_size, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.base(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # CLS\n",
    "        logits = self.classifier(self.dropout(pooled_output))\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        return type('Output', (), {'loss': loss, 'logits': logits})\n",
    "def dataset_to_dataframe(dataset, tokenizer):\n",
    "    records = []\n",
    "    for sample in dataset:\n",
    "        input_ids = sample[\"input_ids\"]\n",
    "        text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "        records.append({\n",
    "            \"case_id\": sample[\"case_id\"],\n",
    "            \"chunk_id\": sample[\"chunk_id\"],\n",
    "            \"label\": sample[\"label\"].item(),\n",
    "            \"chunk_text\": text\n",
    "        })\n",
    "    return pd.DataFrame(records)\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, accuracy_score,\n",
    "    precision_score, recall_score, confusion_matrix\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(y_true, y_prob, y_pred):\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    return {\n",
    "        \"AUC\": auc,\n",
    "        \"F1\": f1,\n",
    "        \"Sensitivity\": recall,\n",
    "        \"Specificity\": specificity,\n",
    "        \"PPV\": precision,\n",
    "        \"NPV\": npv\n",
    "    }\n",
    "\n",
    "def evaluate_case_level(fold_results):\n",
    "    results = {}\n",
    "\n",
    "    # 1. Majority Voting\n",
    "    voting_df = (\n",
    "        fold_results\n",
    "        .assign(pred=(fold_results[\"prob1\"] > 0.5).astype(int))\n",
    "        .groupby(\"cased\")\n",
    "        .agg({\n",
    "            \"true_label\": \"first\",\n",
    "            \"pred\": lambda x: x.value_counts().idxmax()\n",
    "        })\n",
    "        .reset_index()\n",
    "    )\n",
    "    y_true = voting_df[\"true_label\"]\n",
    "    y_pred = voting_df[\"pred\"]\n",
    "    # For AUC we average prob1 by case\n",
    "    prob_df = fold_results.groupby(\"cased\").agg({\"prob1\": \"mean\"}).reset_index()\n",
    "    y_prob = prob_df[\"prob1\"]\n",
    "    results[\"Majority Voting\"] = compute_metrics(y_true, y_prob, y_pred)\n",
    "\n",
    "    # 2. Soft Voting (average prob)\n",
    "    soft_df = (\n",
    "        fold_results\n",
    "        .groupby(\"cased\")\n",
    "        .agg({\n",
    "            \"true_label\": \"first\",\n",
    "            \"prob1\": \"mean\"\n",
    "        })\n",
    "        .reset_index()\n",
    "    )\n",
    "    y_true = soft_df[\"true_label\"]\n",
    "    y_prob = soft_df[\"prob1\"]\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    results[\"Soft Voting\"] = compute_metrics(y_true, y_prob, y_pred)\n",
    "\n",
    "    # 3. Stacking (Logistic Regression)\n",
    "    stack_df = (\n",
    "        fold_results\n",
    "        .groupby(\"cased\")\n",
    "        .agg({\n",
    "            \"true_label\": \"first\",\n",
    "            \"prob0\": \"mean\",\n",
    "            \"prob1\": \"mean\"\n",
    "        })\n",
    "        .reset_index()\n",
    "    )\n",
    "    X = stack_df[[\"prob0\", \"prob1\"]]\n",
    "    y = stack_df[\"true_label\"]\n",
    "\n",
    "    # 5-fold CV predicted probabilities\n",
    "    clf = LogisticRegression()\n",
    "    y_prob = cross_val_predict(clf, X, y, cv=5, method=\"predict_proba\")[:, 1]\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    results[\"Stacking (LR)\"] = compute_metrics(y, y_prob, y_pred)\n",
    "\n",
    "    # Print all results\n",
    "    for method, metrics in results.items():\n",
    "        print(f\"\\n=== {method} ===\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "\n",
    "# Training\n",
    "def train_epoch(model, data_loader, optimizer, scheduler, device, scaler):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "# Evaluation\n",
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    true_labels, pred_probs, cased = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Validation\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            probs = F.softmax(outputs.logits, dim=1)\n",
    "\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_probs.extend(probs.cpu().numpy())\n",
    "            cased.extend(input_ids.cpu().numpy())\n",
    "\n",
    "    return np.array(cased), np.array(true_labels), np.array(pred_probs)\n",
    "\n",
    "# Model list\n",
    "# namess = ['deepseekr1','Cluade-3.7','qwen','chatgpt_4o','gemini1.5_pro','LLama3.3']\n",
    "models = [\n",
    "    'ckiplab/albert-base-chinese',\n",
    "    'indiejoseph/bert-base-cantonese',\n",
    "    'zwzzz/Chinese-MentalBERT',\n",
    "    'Geotrend/distilbert-base-zh-cased',\n",
    "    'hfl/chinese-roberta-wwm-ext',\n",
    "    'hfl/chinese-xlnet-base',\n",
    "    'hfl/chinese-electra-base-discriminator'\n",
    "]\n",
    "\n",
    "names=['text','interviewee_text','new_text','new_interviewee_text']\n",
    "\n",
    "all_model_metrics = []\n",
    "\n",
    "for name in names:\n",
    "    originaldata = pd.read_csv(\"combine.csv\")\n",
    "    texts = originaldata[name].tolist()\n",
    "    labels = originaldata[\"label\"].tolist()\n",
    "    cases = originaldata[\"case\"].tolist()\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for model_name in models:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        result_all_folds = pd.DataFrame()\n",
    "\n",
    "        for fold_idx, (train_index, val_index) in enumerate(skf.split(texts, labels)):\n",
    "            train_texts = [texts[i] for i in train_index]\n",
    "            train_labels = [labels[i] for i in train_index]\n",
    "\n",
    "            val_texts = [texts[i] for i in val_index]\n",
    "            val_labels = [labels[i] for i in val_index]\n",
    "            val_cases = [cases[i] for i in val_index]\n",
    "\n",
    "            train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "            val_dataset = TextDataset(val_texts, val_labels, tokenizer, cases=val_cases)\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, generator=torch.Generator().manual_seed(6))\n",
    "            val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "            try:\n",
    "                model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "            except:\n",
    "                base = AutoModel.from_pretrained(model_name)\n",
    "                model = CustomClassifier(base_model=base, hidden_size=base.config.hidden_size).to(device)\n",
    "\n",
    "            optimizer = AdamW(model.parameters(), lr=1e-5, correct_bias=False)\n",
    "            scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * 5)\n",
    "            scaler = GradScaler()\n",
    "\n",
    "            for epoch in range(20):\n",
    "                print(f\"Model: {model_name}, Fold: {fold_idx + 1}, Epoch {epoch + 1}\")\n",
    "                train_acc, train_loss = train_epoch(model, train_loader, optimizer, scheduler, device, scaler)\n",
    "                print(f\"Train loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "            # Evaluation\n",
    "            cased, true_labels, pred_probs = eval_model(model, val_loader, device)\n",
    "            val_chunks = dataset_to_dataframe(val_dataset, tokenizer)\n",
    "\n",
    "            fold_results = pd.DataFrame({\n",
    "                \"cased\": val_chunks[\"case_id\"],\n",
    "                \"true_label\": val_chunks[\"label\"],\n",
    "                \"prob0\": pred_probs[:, 0],\n",
    "                \"prob1\": pred_probs[:, 1],\n",
    "            })\n",
    "\n",
    "            result_all_folds = pd.concat([result_all_folds, fold_results], ignore_index=True)\n",
    "\n",
    "        # Majority Voting\n",
    "        voting_df = (\n",
    "            result_all_folds\n",
    "            .assign(pred=(result_all_folds[\"prob1\"] > 0.5).astype(int))\n",
    "            .groupby(\"cased\")\n",
    "            .agg({\n",
    "                \"true_label\": \"first\",\n",
    "                \"pred\": lambda x: x.value_counts().idxmax(),\n",
    "                \"prob1\": \"mean\"\n",
    "            })\n",
    "            .reset_index()\n",
    "            .rename(columns={\"cased\": \"case\", \"true_label\": \"label\", \"prob1\": \"prob\"})\n",
    "        )\n",
    "\n",
    "        y_true = voting_df[\"label\"]\n",
    "        y_pred = voting_df[\"pred\"]\n",
    "        y_prob = voting_df[\"prob\"]\n",
    "        y_case = voting_df[\"case\"]\n",
    "        metrics = compute_metrics(y_true, y_prob, y_pred)\n",
    "        model_short = model_name.split(\"/\")[-1]\n",
    "        print(f\"\\n=== {model_short} on {name} ===\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "        metrics[\"Model\"] = model_short\n",
    "        metrics[\"TextType\"] = name\n",
    "        all_model_metrics.append(metrics)\n",
    "\n",
    "        # Save majority voting results\n",
    "#         voting_df.to_csv(f\"./newresult/{name}_{model_short}_majority_voting_results.csv\", index=False)\n",
    "\n",
    "# # Save all metrics\n",
    "# metrics_df = pd.DataFrame(all_model_metrics)\n",
    "# metrics_df.to_csv(\"./all_model_metrics_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "794a5737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1546 > 512). Running this sequence through the model will result in indexing errors\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\xianl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\xianl\\AppData\\Local\\Temp\\ipykernel_16420\\2744621110.py:239: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/94 [00:00<?, ?it/s]C:\\Users\\xianl\\AppData\\Local\\Temp\\ipykernel_16420\\2744621110.py:150: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|██████████| 94/94 [00:47<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5848, Accuracy: 0.7360\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:47<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5556, Accuracy: 0.7527\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:47<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5331, Accuracy: 0.7527\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:48<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5435, Accuracy: 0.7527\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:47<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5101, Accuracy: 0.7527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 23/23 [00:04<00:00,  5.11it/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:53<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5858, Accuracy: 0.7344\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:53<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5589, Accuracy: 0.7477\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:53<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5304, Accuracy: 0.7477\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:53<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5174, Accuracy: 0.7483\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 95/95 [00:53<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4867, Accuracy: 0.7503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 22/22 [00:04<00:00,  5.01it/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:49<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5716, Accuracy: 0.7368\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:49<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5575, Accuracy: 0.7562\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:50<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5510, Accuracy: 0.7569\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:50<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5260, Accuracy: 0.7569\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 91/91 [00:49<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4992, Accuracy: 0.7583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 26/26 [00:05<00:00,  5.07it/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [01:08<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5854, Accuracy: 0.7156\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [01:08<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5641, Accuracy: 0.7407\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [01:08<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5650, Accuracy: 0.7407\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [01:08<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5559, Accuracy: 0.7407\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 93/93 [01:08<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5436, Accuracy: 0.7407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 24/24 [00:04<00:00,  5.10it/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:55<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5811, Accuracy: 0.7335\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:55<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5839, Accuracy: 0.7415\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:56<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5703, Accuracy: 0.7415\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:55<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5607, Accuracy: 0.7415\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 94/94 [00:56<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5268, Accuracy: 0.7415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 23/23 [00:04<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== albert-base-chinese on text ===\n",
      "AUC: 0.7417\n",
      "F1: 0.7771\n",
      "Sensitivity: 1.0000\n",
      "Specificity: 0.0000\n",
      "PPV: 0.6355\n",
      "NPV: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (518 > 512). Running this sequence through the model will result in indexing errors\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\xianl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\xianl\\AppData\\Local\\Temp\\ipykernel_16420\\2744621110.py:239: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/43 [00:00<?, ?it/s]C:\\Users\\xianl\\AppData\\Local\\Temp\\ipykernel_16420\\2744621110.py:150: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|██████████| 43/43 [00:27<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5546, Accuracy: 0.7651\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 43/43 [00:26<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5359, Accuracy: 0.7489\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 43/43 [00:26<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4943, Accuracy: 0.7680\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 43/43 [00:26<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4555, Accuracy: 0.8076\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 43/43 [00:27<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4205, Accuracy: 0.8253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 10/10 [00:02<00:00,  5.00it/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 44/44 [00:32<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5854, Accuracy: 0.7157\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 44/44 [00:32<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5231, Accuracy: 0.7648\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 44/44 [00:32<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5266, Accuracy: 0.7561\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 44/44 [00:32<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5009, Accuracy: 0.7835\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 44/44 [00:32<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4703, Accuracy: 0.7908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 10/10 [00:01<00:00,  5.45it/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 41/41 [00:26<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5472, Accuracy: 0.7601\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 41/41 [00:26<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4829, Accuracy: 0.8006\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 41/41 [00:26<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4642, Accuracy: 0.8022\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 41/41 [00:27<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4642, Accuracy: 0.8131\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 41/41 [00:26<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4261, Accuracy: 0.8209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 13/13 [00:02<00:00,  5.28it/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 43/43 [00:31<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5873, Accuracy: 0.7134\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 43/43 [00:32<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5457, Accuracy: 0.7518\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 43/43 [00:32<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5423, Accuracy: 0.7607\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 43/43 [00:31<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5245, Accuracy: 0.7651\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 43/43 [00:32<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5341, Accuracy: 0.7770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 11/11 [00:02<00:00,  5.42it/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 42/42 [00:25<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5601, Accuracy: 0.7168\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 42/42 [00:25<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5186, Accuracy: 0.7765\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 42/42 [00:25<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5049, Accuracy: 0.7765\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 42/42 [00:25<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4875, Accuracy: 0.7750\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 42/42 [00:25<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4833, Accuracy: 0.7779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 11/11 [00:02<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== albert-base-chinese on interviewee_text ===\n",
      "AUC: 0.7061\n",
      "F1: 0.7765\n",
      "Sensitivity: 0.8684\n",
      "Specificity: 0.3578\n",
      "PPV: 0.7021\n",
      "NPV: 0.6094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2350 > 512). Running this sequence through the model will result in indexing errors\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\xianl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\xianl\\AppData\\Local\\Temp\\ipykernel_16420\\2744621110.py:239: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/82 [00:00<?, ?it/s]C:\\Users\\xianl\\AppData\\Local\\Temp\\ipykernel_16420\\2744621110.py:150: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|██████████| 82/82 [00:45<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5895, Accuracy: 0.7215\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:46<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5702, Accuracy: 0.7353\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:46<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5629, Accuracy: 0.7368\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:45<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5463, Accuracy: 0.7399\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:46<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5225, Accuracy: 0.7429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 18/18 [00:03<00:00,  5.14it/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:51<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6203, Accuracy: 0.6925\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:51<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5961, Accuracy: 0.7202\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:51<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5773, Accuracy: 0.7218\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:51<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5474, Accuracy: 0.7325\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 82/82 [00:51<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5139, Accuracy: 0.7679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 18/18 [00:03<00:00,  5.03it/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 77/77 [00:57<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6031, Accuracy: 0.7281\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 77/77 [00:57<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5820, Accuracy: 0.7379\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 77/77 [00:56<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5666, Accuracy: 0.7379\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 77/77 [00:58<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5336, Accuracy: 0.7379\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 77/77 [00:57<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4910, Accuracy: 0.7838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 23/23 [00:04<00:00,  5.00it/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 79/79 [01:05<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6104, Accuracy: 0.6986\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 79/79 [01:06<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5869, Accuracy: 0.7170\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 79/79 [01:07<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5847, Accuracy: 0.7186\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 79/79 [01:06<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5684, Accuracy: 0.7290\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 79/79 [01:05<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5518, Accuracy: 0.7386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 21/21 [00:04<00:00,  5.00it/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:48<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6287, Accuracy: 0.6822\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:48<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5995, Accuracy: 0.7271\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:48<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5793, Accuracy: 0.7279\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:48<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5753, Accuracy: 0.7279\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [00:47<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5450, Accuracy: 0.7248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 20/20 [00:03<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== albert-base-chinese on new_text ===\n",
      "AUC: 0.6744\n",
      "F1: 0.7667\n",
      "Sensitivity: 0.9684\n",
      "Specificity: 0.0275\n",
      "PPV: 0.6345\n",
      "NPV: 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1475 > 512). Running this sequence through the model will result in indexing errors\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\xianl\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\xianl\\AppData\\Local\\Temp\\ipykernel_16420\\2744621110.py:239: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/43 [00:00<?, ?it/s]C:\\Users\\xianl\\AppData\\Local\\Temp\\ipykernel_16420\\2744621110.py:150: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training: 100%|██████████| 43/43 [00:31<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5786, Accuracy: 0.7137\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 43/43 [00:33<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5630, Accuracy: 0.7442\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 43/43 [00:33<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5564, Accuracy: 0.7442\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 43/43 [00:32<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5430, Accuracy: 0.7442\n",
      "Model: ckiplab/albert-base-chinese, Fold: 1, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 43/43 [00:32<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5285, Accuracy: 0.7442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 10/10 [00:01<00:00,  5.07it/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 45/45 [00:27<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5779, Accuracy: 0.7336\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 45/45 [00:27<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5609, Accuracy: 0.7406\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 45/45 [00:26<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5417, Accuracy: 0.7462\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 45/45 [00:26<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5357, Accuracy: 0.7587\n",
      "Model: ckiplab/albert-base-chinese, Fold: 2, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 45/45 [00:26<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5167, Accuracy: 0.7601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 9/9 [00:01<00:00,  5.63it/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:25<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5609, Accuracy: 0.7203\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:25<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5320, Accuracy: 0.7734\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:25<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5215, Accuracy: 0.7656\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:26<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4814, Accuracy: 0.7828\n",
      "Model: ckiplab/albert-base-chinese, Fold: 3, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 40/40 [00:25<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4464, Accuracy: 0.7828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 13/13 [00:02<00:00,  5.06it/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 42/42 [00:23<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5869, Accuracy: 0.7046\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 42/42 [00:23<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5937, Accuracy: 0.7241\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 42/42 [00:23<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5914, Accuracy: 0.7256\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 42/42 [00:23<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5852, Accuracy: 0.7256\n",
      "Model: ckiplab/albert-base-chinese, Fold: 4, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 42/42 [00:23<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5814, Accuracy: 0.7256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 12/12 [00:02<00:00,  5.39it/s]\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-base-chinese and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 42/42 [00:23<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5864, Accuracy: 0.7321\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 42/42 [00:22<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5631, Accuracy: 0.7351\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 42/42 [00:22<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5161, Accuracy: 0.7619\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 42/42 [00:23<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4402, Accuracy: 0.7932\n",
      "Model: ckiplab/albert-base-chinese, Fold: 5, Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 42/42 [00:22<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3436, Accuracy: 0.8586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 11/11 [00:02<00:00,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== albert-base-chinese on new_interviewee_text ===\n",
      "AUC: 0.6537\n",
      "F1: 0.7748\n",
      "Sensitivity: 0.9053\n",
      "Specificity: 0.2477\n",
      "PPV: 0.6772\n",
      "NPV: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    AutoModel, AdamW, get_scheduler, set_seed\n",
    ")\n",
    "from torch.nn import functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from opencc import OpenCC\n",
    "import jieba\n",
    "\n",
    "# --- Seed setup ---\n",
    "def seed_everything(seed=6):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(6)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def tokenize_cantonese(text):\n",
    "    return list(jieba.cut(text))\n",
    "# --- Preprocessing function using Weikit and OpenCC ---\n",
    "def preprocess_cantonese(texts, opencc_config='hk2s'):\n",
    "    cc = OpenCC(opencc_config)\n",
    "    processed = []\n",
    "    for text in texts:\n",
    "        text_conv = cc.convert(text)  # Convert to Mandarin/Simplified\n",
    "        tokens = tokenize_cantonese(text_conv)  # Tokenize (still useful for segmentation)\n",
    "        processed.append(\" \".join(tokens))\n",
    "    return processed\n",
    "\n",
    "# --- Dataset ---\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512, overlap=128, cases=None, pad_token_id=None):\n",
    "        self.samples = []\n",
    "        pad_token_id = pad_token_id if pad_token_id is not None else tokenizer.pad_token_id or 0\n",
    "        for idx, (text, label) in enumerate(zip(texts, labels)):\n",
    "            case_id = cases[idx] if cases is not None else idx\n",
    "            encoding = tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                return_attention_mask=False,\n",
    "                return_tensors=None,\n",
    "            )\n",
    "            input_ids = encoding[\"input_ids\"]\n",
    "            # Chunking\n",
    "            start = 0\n",
    "            while start < len(input_ids):\n",
    "                end = start + max_len\n",
    "                chunk = input_ids[start:end]\n",
    "                if len(chunk) < max_len:\n",
    "                    chunk += [pad_token_id] * (max_len - len(chunk))\n",
    "                self.samples.append({\n",
    "                    \"input_ids\": chunk,\n",
    "                    \"label\": label,\n",
    "                    \"case_id\": case_id,\n",
    "                    \"chunk_id\": start // (max_len - overlap)\n",
    "                })\n",
    "                if end >= len(input_ids):\n",
    "                    break\n",
    "                start += max_len - overlap\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        input_ids = torch.tensor(sample[\"input_ids\"], dtype=torch.long)\n",
    "        attention_mask = (input_ids != 0).long()  # assumes pad_token_id is 0\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"label\": torch.tensor(sample[\"label\"], dtype=torch.long),\n",
    "            \"case_id\": sample[\"case_id\"],\n",
    "            \"chunk_id\": sample[\"chunk_id\"],\n",
    "        }\n",
    "\n",
    "# --- Model fallback if classification head is missing ---\n",
    "class CustomClassifier(torch.nn.Module):\n",
    "    def __init__(self, base_model, hidden_size, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.base(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # CLS token\n",
    "        logits = self.classifier(self.dropout(pooled_output))\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        return type('Output', (), {'loss': loss, 'logits': logits})\n",
    "\n",
    "def dataset_to_dataframe(dataset, tokenizer):\n",
    "    records = []\n",
    "    for sample in dataset:\n",
    "        input_ids = sample[\"input_ids\"]\n",
    "        text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "        records.append({\n",
    "            \"case_id\": sample[\"case_id\"],\n",
    "            \"chunk_id\": sample[\"chunk_id\"],\n",
    "            \"label\": sample[\"label\"].item(),\n",
    "            \"chunk_text\": text\n",
    "        })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# --- Metrics ---\n",
    "def compute_metrics(y_true, y_prob, y_pred):\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    return {\n",
    "        \"AUC\": auc,\n",
    "        \"F1\": f1,\n",
    "        \"Sensitivity\": recall,\n",
    "        \"Specificity\": specificity,\n",
    "        \"PPV\": precision,\n",
    "        \"NPV\": npv\n",
    "    }\n",
    "\n",
    "# --- Training ---\n",
    "def train_epoch(model, data_loader, optimizer, scheduler, device, scaler):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "# --- Evaluation ---\n",
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    true_labels, pred_probs, cased = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Validation\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            probs = F.softmax(outputs.logits, dim=1)\n",
    "\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_probs.extend(probs.cpu().numpy())\n",
    "            # FIX HERE:\n",
    "            cased.extend(batch[\"case_id\"])  # No .cpu().numpy() needed\n",
    "    return np.array(cased), np.array(true_labels), np.array(pred_probs)\n",
    "\n",
    "# --- Model list ---\n",
    "models = [\n",
    "    'ckiplab/albert-base-chinese',\n",
    "    # 'indiejoseph/bert-base-cantonese',\n",
    "    # 'zwzzz/Chinese-MentalBERT',\n",
    "    # 'Geotrend/distilbert-base-zh-cased',\n",
    "    # 'hfl/chinese-roberta-wwm-ext',\n",
    "    # 'hfl/chinese-xlnet-base',\n",
    "    # 'hfl/chinese-electra-base-discriminator'\n",
    "]\n",
    "\n",
    "names=['text','interviewee_text','new_text','new_interviewee_text']\n",
    "\n",
    "all_model_metrics = []\n",
    "\n",
    "for name in names:\n",
    "    originaldata = pd.read_csv(\"combine.csv\")\n",
    "    raw_texts = originaldata[name].tolist()\n",
    "    labels = originaldata[\"label\"].tolist()\n",
    "    cases = originaldata[\"case\"].tolist()\n",
    "\n",
    "    # --- Preprocess using Weikit + OpenCC for Cantonese ---\n",
    "    texts = preprocess_cantonese(raw_texts, opencc_config='hk2s')  # or 't2s', depending on your text\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for model_name in models:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        result_all_folds = pd.DataFrame()\n",
    "\n",
    "        for fold_idx, (train_index, val_index) in enumerate(skf.split(texts, labels)):\n",
    "            train_texts = [texts[i] for i in train_index]\n",
    "            train_labels = [labels[i] for i in train_index]\n",
    "            val_texts = [texts[i] for i in val_index]\n",
    "            val_labels = [labels[i] for i in val_index]\n",
    "            val_cases = [cases[i] for i in val_index]\n",
    "\n",
    "            train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "            val_dataset = TextDataset(val_texts, val_labels, tokenizer, cases=val_cases)\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, generator=torch.Generator().manual_seed(6))\n",
    "            val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "            try:\n",
    "                model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "            except:\n",
    "                base = AutoModel.from_pretrained(model_name)\n",
    "                model = CustomClassifier(base_model=base, hidden_size=base.config.hidden_size).to(device)\n",
    "\n",
    "            optimizer = AdamW(model.parameters(), lr=1e-5, correct_bias=False)\n",
    "            scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * 5)\n",
    "            scaler = GradScaler()\n",
    "\n",
    "            for epoch in range(5):  # Reduce epochs for speed; adjust as needed\n",
    "                print(f\"Model: {model_name}, Fold: {fold_idx + 1}, Epoch {epoch + 1}\")\n",
    "                train_acc, train_loss = train_epoch(model, train_loader, optimizer, scheduler, device, scaler)\n",
    "                print(f\"Train loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "            # Evaluation\n",
    "            cased, true_labels, pred_probs = eval_model(model, val_loader, device)\n",
    "            val_chunks = dataset_to_dataframe(val_dataset, tokenizer)\n",
    "\n",
    "            fold_results = pd.DataFrame({\n",
    "                \"cased\": val_chunks[\"case_id\"],\n",
    "                \"true_label\": val_chunks[\"label\"],\n",
    "                \"prob0\": pred_probs[:, 0],\n",
    "                \"prob1\": pred_probs[:, 1],\n",
    "            })\n",
    "\n",
    "            result_all_folds = pd.concat([result_all_folds, fold_results], ignore_index=True)\n",
    "\n",
    "        # Majority Voting\n",
    "        voting_df = (\n",
    "            result_all_folds\n",
    "            .assign(pred=(result_all_folds[\"prob1\"] > 0.5).astype(int))\n",
    "            .groupby(\"cased\")\n",
    "            .agg({\n",
    "                \"true_label\": \"first\",\n",
    "                \"pred\": lambda x: x.value_counts().idxmax(),\n",
    "                \"prob1\": \"mean\"\n",
    "            })\n",
    "            .reset_index()\n",
    "            .rename(columns={\"cased\": \"case\", \"true_label\": \"label\", \"prob1\": \"prob\"})\n",
    "        )\n",
    "\n",
    "        y_true = voting_df[\"label\"]\n",
    "        y_pred = voting_df[\"pred\"]\n",
    "        y_prob = voting_df[\"prob\"]\n",
    "        y_case = voting_df[\"case\"]\n",
    "        metrics = compute_metrics(y_true, y_prob, y_pred)\n",
    "        model_short = model_name.split(\"/\")[-1]\n",
    "        print(f\"\\n=== {model_short} on {name} ===\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "        metrics[\"Model\"] = model_short\n",
    "        metrics[\"TextType\"] = name\n",
    "        all_model_metrics.append(metrics)\n",
    "\n",
    "        # Optionally save results for each model\n",
    "        # voting_df.to_csv(f\"./newresult/{name}_{model_short}_majority_voting_results.csv\", index=False)\n",
    "\n",
    "# # Save all metrics summary\n",
    "# metrics_df = pd.DataFrame(all_model_metrics)\n",
    "# metrics_df.to_csv(\"./all_model_metrics_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5739514e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AUC': np.float64(0.7530661516175761),\n",
       " 'F1': 0.7903930131004366,\n",
       " 'Sensitivity': 0.9526315789473684,\n",
       " 'Specificity': np.float64(0.2018348623853211),\n",
       " 'PPV': 0.6753731343283582,\n",
       " 'NPV': np.float64(0.7096774193548387)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(y_true, y_prob, y_pred):\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    return {\n",
    "        \"AUC\": auc,\n",
    "        \"F1\": f1,\n",
    "        \"Sensitivity\": recall,\n",
    "        \"Specificity\": specificity,\n",
    "        \"PPV\": precision,\n",
    "        \"NPV\": npv\n",
    "    }\n",
    "data=pd.read_csv('./newresult/text_albert-base-chinese_majority_voting_results.csv')\n",
    "compute_metrics(data['label'], data['prob'], data['pred'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
